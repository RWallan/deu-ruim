"""Handle the LLM agents."""

import os

from pydantic_ai import Agent, ModelRetry, RunContext
from pydantic_ai.format_as_xml import format_as_xml
from pydantic_ai.models.openai import OpenAIModel

from .deps import FixerDeps
from .schemas import FixerFail, FixerResponse, ValidatorFail, ValidatorResponse

m = OpenAIModel(
    model_name='qwen2.5-coder',  # TODO: make dynamic
    api_key=os.getenv('OLLAMA_API_KEY'),
    base_url=f'{os.getenv("OLLAMA_API_URL")}/v1',
)

Validator: Agent[None, ValidatorResponse] = Agent(
    m,
    result_type=ValidatorResponse,  # type: ignore
)


@Validator.system_prompt
async def validator_system_prompt() -> str:
    return """\
You'll receive a terminal command generated by a LLM agent.
Be concise and return True if the command is valid else False.
"""


Fixer: Agent[FixerDeps, FixerResponse] = Agent(
    m,
    result_type=FixerResponse,  # type: ignore
    deps_type=FixerDeps,
    retries=2,
)


@Fixer.system_prompt
async def fixer_system_prompt() -> str:
    command_examples = [
        {'request': 'sl', 'response': 'ls'},
        {'request': 'cd..', 'response': 'cd ..'},
        {'request': 'esec bash', 'response': 'exec bash'},
    ]

    return f"""\
    Be concise and only return the fixed command.

    Answer without any explanation.

    {format_as_xml(command_examples)}
    """


@Fixer.result_validator  # type: ignore
async def validate_result(
    ctx: RunContext[FixerDeps], response: FixerResponse
) -> FixerResponse:
    """Check if the command generated runs correctly.

    Args:
        ctx: Run context
        response: LLM response

    Returns:
        Validated LLM response
    """
    if isinstance(response, FixerFail):
        return response

    validator_response = await ctx.deps.validator_agent.run(
        response.fixed_command
    )

    if isinstance(validator_response.data, ValidatorFail):
        raise ModelRetry('Please, generate a valid command')

    return response
